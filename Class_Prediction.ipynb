{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vingu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vingu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Original course: C S 329E Data Analytics TR 9:30 a.m.-11:00 a.m. Instructor XYZ\n",
      "\n",
      "Recommendations using Cosine Similarity:\n",
      "                                    NAME   DAYS STARTTIME ENDTIME\n",
      "87   C S 329E ELEMENTS OF DATA ANALYTICS    TTH      8:30   10:00\n",
      "95   C S 330E ELMNTS SOFTWARE ENGINEER I     MW      9:30   11:00\n",
      "5   C S 309 AI LITERACY: ESSENT OF AI-WB    TTH      9:30   11:00\n",
      "96   C S 330E ELMNTS SOFTWARE ENGINEER I     MW     11:00   12:30\n",
      "77        C S 327E ELEMENTS OF DATABASES    TTH      9:30   11:00\n",
      "28   C S 312 INTRODUCTION TO PROGRAMMING  MWF M      9:00   10:00\n",
      "27   C S 312 INTRODUCTION TO PROGRAMMING  MWF M      9:00   10:00\n",
      "\n",
      "Recommendations using K-Means Clustering:\n",
      "                                    NAME   DAYS STARTTIME ENDTIME\n",
      "87   C S 329E ELEMENTS OF DATA ANALYTICS    TTH      8:30   10:00\n",
      "95   C S 330E ELMNTS SOFTWARE ENGINEER I     MW      9:30   11:00\n",
      "5   C S 309 AI LITERACY: ESSENT OF AI-WB    TTH      9:30   11:00\n",
      "96   C S 330E ELMNTS SOFTWARE ENGINEER I     MW     11:00   12:30\n",
      "77        C S 327E ELEMENTS OF DATABASES    TTH      9:30   11:00\n",
      "28   C S 312 INTRODUCTION TO PROGRAMMING  MWF M      9:00   10:00\n",
      "27   C S 312 INTRODUCTION TO PROGRAMMING  MWF M      9:00   10:00\n",
      "\n",
      "Recommendations using Autoencoder:\n",
      "                                    NAME DAYS STARTTIME ENDTIME\n",
      "95   C S 330E ELMNTS SOFTWARE ENGINEER I   MW      9:30   11:00\n",
      "96   C S 330E ELMNTS SOFTWARE ENGINEER I   MW     11:00   12:30\n",
      "87   C S 329E ELEMENTS OF DATA ANALYTICS  TTH      8:30   10:00\n",
      "71       C S 326E ELEMENTS OF NETWORKING  TTH     11:00   12:30\n",
      "5   C S 309 AI LITERACY: ESSENT OF AI-WB  TTH      9:30   11:00\n",
      "72       C S 326E ELEMENTS OF NETWORKING  TTH     12:30    2:00\n",
      "77        C S 327E ELEMENTS OF DATABASES  TTH      9:30   11:00\n",
      "\n",
      "Recommendations using DBSCAN:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['NAME', 'DAYS', 'STARTTIME', 'ENDTIME'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecommendations using DBSCAN:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m recommendations_dbscan \u001b[38;5;241m=\u001b[39m recommend_dbscan(test_course, embeddings, df_courses)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrecommendations_dbscan\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDAYS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTARTTIME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mENDTIME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\vingu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vingu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vingu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['NAME', 'DAYS', 'STARTTIME', 'ENDTIME'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Read the JSON file\n",
    "with open('courses.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the data from the 'value' key\n",
    "course_data = data['value']\n",
    "\n",
    "# Convert course data to DataFrame\n",
    "df_courses = pd.DataFrame(course_data)\n",
    "\n",
    "# Select important attributes for embeddings\n",
    "df_courses['embedding_text'] = df_courses.apply(\n",
    "    lambda row: f\"{row['NAME']} {row['PROFESSOR']} {row['DAYS']} {row['STARTTIME']} {row['ENDTIME']} {row['ROOM']} {row['INSTRUCTION_MODE']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Generate embeddings for the selected attributes\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(df_courses['embedding_text'].tolist())\n",
    "embeddings = np.array(embeddings, dtype=np.float64)\n",
    "\n",
    "# Define the test course to replace\n",
    "test_course = \"C S 329E Data Analytics TR 9:30 a.m.-11:00 a.m. Instructor XYZ\"\n",
    "\n",
    "# Function to recommend courses using Cosine Similarity\n",
    "def recommend_cosine(course, embeddings, courses_df, top_n=7):\n",
    "    course_embedding = model.encode([course])[0]\n",
    "    similarities = cosine_similarity([course_embedding], embeddings)[0]\n",
    "    indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    return courses_df.iloc[indices]\n",
    "\n",
    "# Function to recommend courses using K-Means Clustering\n",
    "def recommend_kmeans(course, embeddings, courses_df, top_n=7, num_clusters=5):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.predict(embeddings)\n",
    "    course_embedding = model.encode([course])[0]\n",
    "    course_embedding = np.array(course_embedding, dtype=np.float64)  # Ensure dtype consistency\n",
    "    cluster = kmeans.predict([course_embedding])[0]\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    similarities = cosine_similarity([course_embedding], embeddings[cluster_indices])[0]\n",
    "    sorted_cluster_indices = cluster_indices[np.argsort(similarities)[::-1][1:top_n+1]]\n",
    "    return courses_df.iloc[sorted_cluster_indices]\n",
    "\n",
    "# Function to recommend courses using Autoencoder\n",
    "def recommend_autoencoder(course, embeddings, courses_df, top_n=7):\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    course_embedding = scaler.transform(model.encode([course]))\n",
    "\n",
    "    autoencoder = MLPRegressor(hidden_layer_sizes=(64, 32, 64), max_iter=5000, random_state=42)\n",
    "    autoencoder.fit(embeddings_scaled, embeddings_scaled)\n",
    "\n",
    "    course_embedding_pred = autoencoder.predict(course_embedding)\n",
    "    similarities = cosine_similarity(course_embedding_pred, embeddings_scaled)[0]\n",
    "    indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    return courses_df.iloc[indices]\n",
    "\n",
    "\n",
    "def recommend_dbscan(course, embeddings, courses_df, top_n=7, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(embeddings)\n",
    "    \n",
    "    # Find the cluster of the given course\n",
    "    course_embedding = model.encode([course])[0]\n",
    "    course_embedding = np.array(course_embedding, dtype=np.float64)\n",
    "    cluster_labels = dbscan.fit_predict(np.vstack([embeddings, course_embedding]))\n",
    "\n",
    "    # Check if the course is noise\n",
    "    course_cluster = cluster_labels[-1]\n",
    "    if course_cluster == -1:  # If the course is considered noise\n",
    "        return pd.DataFrame()  # Or handle noise appropriately\n",
    "\n",
    "    # Get indices of courses in the same cluster\n",
    "    cluster_indices = np.where(clusters == course_cluster)[0]\n",
    "    similarities = cosine_similarity([course_embedding], embeddings[cluster_indices])[0]\n",
    "    sorted_cluster_indices = cluster_indices[np.argsort(similarities)[::-1][1:top_n+1]]\n",
    "    return courses_df.iloc[sorted_cluster_indices]\n",
    "\n",
    "# Print original course\n",
    "print(\"Original course:\", test_course)\n",
    "\n",
    "# Generate recommendations using different methods\n",
    "print(\"\\nRecommendations using Cosine Similarity:\")\n",
    "recommendations_cosine = recommend_cosine(test_course, embeddings, df_courses)\n",
    "print(recommendations_cosine[['NAME', 'DAYS', 'STARTTIME', 'ENDTIME']])\n",
    "\n",
    "print(\"\\nRecommendations using K-Means Clustering:\")\n",
    "recommendations_kmeans = recommend_kmeans(test_course, embeddings, df_courses)\n",
    "print(recommendations_kmeans[['NAME', 'DAYS', 'STARTTIME', 'ENDTIME']])\n",
    "\n",
    "print(\"\\nRecommendations using Autoencoder:\")\n",
    "recommendations_autoencoder = recommend_autoencoder(test_course, embeddings, df_courses)\n",
    "print(recommendations_autoencoder[['NAME', 'DAYS', 'STARTTIME', 'ENDTIME']])\n",
    "\n",
    "print(\"\\nRecommendations using DBSCAN:\")\n",
    "recommendations_dbscan = recommend_dbscan(test_course, embeddings, df_courses)\n",
    "print(recommendations_dbscan[['NAME', 'DAYS', 'STARTTIME', 'ENDTIME']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Sample course dataset\n",
    "courses = [\n",
    "    \"Introduction to Computer Science\",\n",
    "    \"Advanced Data Structures\",\n",
    "    \"Machine Learning\",\n",
    "    \"Database Systems\",\n",
    "    \"Algorithms\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Software Engineering\",\n",
    "    \"Computer Networks\",\n",
    "    \"Operating Systems\",\n",
    "    \"Computer Graphics\",\n",
    "    \"Data Mining\"\n",
    "]\n",
    "\n",
    "# Load pre-trained model for embeddings\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(courses)\n",
    "\n",
    "# Ensure embeddings are of type float64 (double)\n",
    "embeddings = np.array(embeddings, dtype=np.float64)\n",
    "\n",
    "# Save embeddings and course names\n",
    "with open('course_embeddings.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'courses': courses,\n",
    "        'embeddings': embeddings.tolist()\n",
    "    }, f)\n",
    "\n",
    "# Function to recommend courses based on cosine similarity\n",
    "def recommend_cosine(course, embeddings, courses, top_n=3):\n",
    "    course_embedding = model.encode([course])[0]\n",
    "    similarities = cosine_similarity([course_embedding], embeddings)[0]\n",
    "    indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    return [courses[i] for i in indices]\n",
    "\n",
    "# Function to recommend courses using clustering (K-Means)\n",
    "def recommend_kmeans(course, embeddings, courses, top_n=3, num_clusters=5):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.predict(embeddings)\n",
    "    course_embedding = model.encode([course])[0]\n",
    "    course_embedding = np.array(course_embedding, dtype=np.float64)  # Ensure dtype consistency\n",
    "    cluster = kmeans.predict([course_embedding])[0]\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    similarities = cosine_similarity([course_embedding], embeddings[cluster_indices])[0]\n",
    "    sorted_cluster_indices = cluster_indices[np.argsort(similarities)[::-1][1:top_n+1]]\n",
    "    return [courses[i] for i in sorted_cluster_indices]\n",
    "\n",
    "# Function to recommend courses using neural network (Autoencoder)\n",
    "def recommend_autoencoder(course, embeddings, courses, top_n=3):\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    course_embedding = scaler.transform(model.encode([course]))\n",
    "\n",
    "    autoencoder = MLPRegressor(hidden_layer_sizes=(64, 32, 64), max_iter=5000, random_state=42)\n",
    "    autoencoder.fit(embeddings_scaled, embeddings_scaled)\n",
    "\n",
    "    course_embedding_pred = autoencoder.predict(course_embedding)\n",
    "    similarities = cosine_similarity(course_embedding_pred, embeddings_scaled)[0]\n",
    "    indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    return [courses[i] for i in indices]\n",
    "\n",
    "# Test the different methods\n",
    "test_course = \"Advanced Data Structures\"\n",
    "\n",
    "print(\"Original course:\", test_course)\n",
    "print(\"\\nRecommendations using Cosine Similarity:\")\n",
    "print(recommend_cosine(test_course, embeddings, courses))\n",
    "\n",
    "print(\"\\nRecommendations using K-Means Clustering:\")\n",
    "print(recommend_kmeans(test_course, embeddings, courses))\n",
    "\n",
    "print(\"\\nRecommendations using Autoencoder:\")\n",
    "print(recommend_autoencoder(test_course, embeddings, courses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
